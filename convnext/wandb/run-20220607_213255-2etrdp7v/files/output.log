CattleNet(
  (convnext_tiny): Sequential(
    (0): ConvNeXt(
      (features): Sequential(
        (0): ConvNormActivation(
          (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
          (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)
        )
        (1): Sequential(
          (0): CNBlock(
            (block): Sequential(
              (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
              (1): Permute()
              (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=96, out_features=384, bias=True)
              (4): GELU()
              (5): Linear(in_features=384, out_features=96, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.0, mode=row)
          )
          (1): CNBlock(
            (block): Sequential(
              (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
              (1): Permute()
              (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=96, out_features=384, bias=True)
              (4): GELU()
              (5): Linear(in_features=384, out_features=96, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)
          )
          (2): CNBlock(
            (block): Sequential(
              (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
              (1): Permute()
              (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=96, out_features=384, bias=True)
              (4): GELU()
              (5): Linear(in_features=384, out_features=96, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)
          )
        )
        (2): Sequential(
          (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)
          (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
        )
        (3): Sequential(
          (0): CNBlock(
            (block): Sequential(
              (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
              (1): Permute()
              (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=192, out_features=768, bias=True)
              (4): GELU()
              (5): Linear(in_features=768, out_features=192, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)
          )
          (1): CNBlock(
            (block): Sequential(
              (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
              (1): Permute()
              (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=192, out_features=768, bias=True)
              (4): GELU()
              (5): Linear(in_features=768, out_features=192, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)
          )
          (2): CNBlock(
            (block): Sequential(
              (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
              (1): Permute()
              (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=192, out_features=768, bias=True)
              (4): GELU()
              (5): Linear(in_features=768, out_features=192, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)
          )
        )
        (4): Sequential(
          (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
          (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
        )
        (5): Sequential(
          (0): CNBlock(
            (block): Sequential(
              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (1): Permute()
              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=384, out_features=1536, bias=True)
              (4): GELU()
              (5): Linear(in_features=1536, out_features=384, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)
          )
          (1): CNBlock(
            (block): Sequential(
              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (1): Permute()
              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=384, out_features=1536, bias=True)
              (4): GELU()
              (5): Linear(in_features=1536, out_features=384, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)
          )
          (2): CNBlock(
            (block): Sequential(
              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (1): Permute()
              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=384, out_features=1536, bias=True)
              (4): GELU()
              (5): Linear(in_features=1536, out_features=384, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)
          )
          (3): CNBlock(
            (block): Sequential(
              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (1): Permute()
              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=384, out_features=1536, bias=True)
              (4): GELU()
              (5): Linear(in_features=1536, out_features=384, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)
          )
          (4): CNBlock(
            (block): Sequential(
              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (1): Permute()
              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=384, out_features=1536, bias=True)
              (4): GELU()
              (5): Linear(in_features=1536, out_features=384, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)
          )
          (5): CNBlock(
            (block): Sequential(
              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (1): Permute()
              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=384, out_features=1536, bias=True)
              (4): GELU()
              (5): Linear(in_features=1536, out_features=384, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)
          )
          (6): CNBlock(
            (block): Sequential(
              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (1): Permute()
              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=384, out_features=1536, bias=True)
              (4): GELU()
              (5): Linear(in_features=1536, out_features=384, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)
          )
          (7): CNBlock(
            (block): Sequential(
              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (1): Permute()
              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=384, out_features=1536, bias=True)
              (4): GELU()
              (5): Linear(in_features=1536, out_features=384, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)
          )
          (8): CNBlock(
            (block): Sequential(
              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (1): Permute()
              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=384, out_features=1536, bias=True)
              (4): GELU()
              (5): Linear(in_features=1536, out_features=384, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)
          )
        )
        (6): Sequential(
          (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
          (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
        )
        (7): Sequential(
          (0): CNBlock(
            (block): Sequential(
              (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
              (1): Permute()
              (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=768, out_features=3072, bias=True)
              (4): GELU()
              (5): Linear(in_features=3072, out_features=768, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)
          )
          (1): CNBlock(
            (block): Sequential(
              (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
              (1): Permute()
              (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=768, out_features=3072, bias=True)
              (4): GELU()
              (5): Linear(in_features=3072, out_features=768, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)
          )
          (2): CNBlock(
            (block): Sequential(
              (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
              (1): Permute()
              (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (3): Linear(in_features=768, out_features=3072, bias=True)
              (4): GELU()
              (5): Linear(in_features=3072, out_features=768, bias=True)
              (6): Permute()
            )
            (stochastic_depth): StochasticDepth(p=0.1, mode=row)
          )
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=1)
      (classifier): Sequential(
        (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)
        (1): Flatten(start_dim=1, end_dim=-1)
        (2): Linear(in_features=768, out_features=4096, bias=True)
      )
    )
    (1): ReLU()
    (2): Linear(in_features=4096, out_features=2048, bias=True)
    (3): ReLU()
    (4): Linear(in_features=2048, out_features=1024, bias=True)
    (5): ReLU()
    (6): Linear(in_features=1024, out_features=256, bias=True)
    (7): ReLU()
    (8): Linear(in_features=256, out_features=64, bias=True)
    (9): Sigmoid()
  )
)
Starting training



















Epoch [1/200]:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎      | 18/19 [00:41<00:02,  2.35s/it, loss=0.238]
lr 0.00099
Epoch 1

 Current loss 0.19384950895615446
Zero precision: [0, 0, 0, 0, 0]
Zero recall: [0, 0, 0, 0, 0]
Zero acc: [0, 0, 0, 0, 0]
--------
 (( 0.9999960064888 ))
 0.999656081199646
 0.9999793767929077
 0.9991721510887146
 0.9999943971633911
 0.9995770454406738
 0.9998805522918701
 0.9986375570297241
 0.9999596476554871
 0.9986357092857361
 0.999910295009613
 0.999951183795929
 ||| 0.9999969005584717 |||
 0.9994322061538696
 0.9994535446166992
 0.999943196773529
 0.9999261498451233
 0.9999094009399414
 0.9999927878379822
 0.9997713565826416
 0.9998489618301392
 0.999991774559021
 0.9997475743293762
 0.9984822869300842
I
----------
--------
 0.9997633695602417
 (( 0.9999979734420776 ))
 0.9998352527618408
 0.999869704246521
 0.9997212886810303
 0.9999896883964539
 0.9999855756759644
 0.9999916553497314
 0.9997475743293762
 0.9999974966049194
 0.9999979734420776
 0.9998917579650879
 0.9996622204780579
 0.9999585747718811
 0.9993255734443665
 0.9999361038208008
 0.9999224543571472
 0.9999943971633911
 0.9997845888137817
 0.999991238117218
 0.9999822974205017
 0.9999141097068787
 0.999997615814209
 0.9999955296516418
C
----------
--------
 ||| 0.9999774694442749 |||
 0.9992051720619202
 (( 0.9998586773872375 ))
 0.9991788864135742
 0.9998873472213745
 0.9995161294937134
 0.9989888072013855
 0.9994665384292603
 0.9999582171440125
 0.9984551072120667
 0.9998591542243958
 0.9999120235443115
 0.999496340751648
 0.9989669919013977
 0.9991676807403564
 0.9992139935493469
 0.9991927742958069
 0.9996945858001709
 0.9998966455459595
 0.999692976474762
 0.9995207786560059
 0.9999240636825562
 0.9995833039283752
 0.9995309710502625
I
----------
--------
 0.9993104338645935
 0.9999446272850037
 0.99925297498703
 (( 0.9999929070472717 ))
 0.9995192885398865
 0.9999499917030334
 0.9999851584434509
 0.9998784065246582
 0.9996033310890198
 0.9999729990959167
 0.9998440146446228
 0.9996262192726135
 0.9990530610084534
 0.9999877214431763
 0.9999406337738037
 0.9996474385261536
 ||| 0.9999988675117493 |||
 0.9999563694000244
 0.999221920967102
 0.9998578429222107
 0.9999496936798096
 0.9997416138648987
 0.9997281432151794
 0.9999450445175171
I
----------
--------
 0.9999910593032837
 0.999506950378418
 0.9999327063560486
 0.9989480972290039
 (( 0.9998159408569336 ))
 0.999458909034729
 0.9997873306274414
 0.998357355594635
 ||| 0.9999995231628418 |||
 0.9995961785316467
 0.9998053908348083
 0.999919056892395
 0.9999930262565613
 0.9991965889930725
 0.9996075630187988
 0.9998290538787842
 0.9991199970245361
 0.9995789527893066
 0.9998471140861511
 0.9996472001075745
 0.9993636608123779
 0.9999990463256836
 0.9997027516365051
 0.9981892704963684
I
----------
--------
 0.9995770454406738
 0.9999947547912598
 0.9995315670967102
 0.9999648332595825
 0.9998467564582825
 (( 0.9999985694885254 ))
 0.9999080300331116
 0.9998866319656372
 0.9999786019325256
 0.9999969601631165
 0.9999536275863647
 0.9994115829467773
 0.9999858736991882
 0.9999895095825195
 0.9999967217445374
 0.9998300671577454
 0.9999922513961792
 0.9999465942382812
 0.9996809959411621
 0.9999816417694092
 0.9999278783798218
 0.9994509220123291
 0.9999865889549255
 0.999629557132721
C
----------
--------
 0.9998805522918701
 0.9999274611473083
 0.9998558163642883
 0.9997597336769104
 0.9999556541442871
 0.9999061226844788
 (( 0.9996432662010193 ))
 0.9999226331710815
 0.9998077154159546
 0.9999620914459229
 0.9999564290046692
 0.9999843835830688
 0.9997614026069641
 0.999809741973877
 0.9999723434448242
 0.9999977350234985
 0.9999789595603943
 0.999972939491272
 ||| 0.9999983906745911 |||
 0.9999281167984009
 0.9999263286590576
 0.9998193383216858
 0.9999864101409912
 0.9991843700408936
I
----------
--------
 0.9993637800216675
 0.9999626874923706
 0.9994859099388123
 ||| 0.9999970197677612 |||
 0.9992958307266235
 0.999940812587738
 0.9998143911361694
 (( 0.9999950528144836 ))
 0.99964839220047
 0.9998766779899597
 0.99993896484375
 0.999590277671814
 0.9991604685783386
 0.9999868869781494
 0.9999017715454102
 0.9999764561653137
 0.999714195728302
 0.9998928308486938
 0.9995386004447937
 0.9999378323554993
 0.9999396800994873
 0.9995631575584412
 0.999871551990509
 0.999825119972229
I
----------
--------
 0.9999954104423523
 0.999638020992279
 ||| 0.9999991059303284 |||
 0.999506950378418
 0.9999865293502808
 0.9997555613517761
 0.9993578791618347
 0.9995196461677551
 (( 0.9999942183494568 ))
 0.9998468160629272
 0.9999698996543884
 0.9999399781227112
 0.9999752640724182
 0.9997447729110718
 0.999672532081604
 0.9999867081642151
 0.9998675584793091
 0.9998348355293274
 0.9999674558639526
 0.999876856803894
 0.9997591972351074
 0.999992847442627
 0.9999293684959412
 0.998783528804779
I
----------
--------
 0.9996439814567566
 0.999975323677063
 0.9995761513710022
 0.9999324679374695
 0.999807596206665
 0.9999943971633911
 0.9999788403511047
 0.9999402165412903
 0.99989253282547
 (( 0.9999955892562866 ))
 0.9999133348464966
 0.9998742938041687
 0.9996355175971985
 0.9998648166656494
 0.9999977946281433
 0.9999274611473083
 0.9997579455375671
 0.9999123811721802
 0.9995749592781067
 0.9999151825904846
 ||| 0.9999985098838806 |||
 0.9998980760574341
 0.9999985098838806
 0.9999983310699463
I
----------
--------
 0.9998488426208496
 0.999947726726532
 0.9999769330024719
 0.9997900724411011
 0.9999009370803833
 0.9999704957008362
 ||| 0.9999991655349731 |||
 0.9997172355651855
 0.9999808669090271
 0.9994749426841736
 (( 0.9999766945838928 ))
 0.9996768832206726
 0.9997460842132568
 0.9999080896377563
 0.9999339580535889
 0.9999754428863525
 0.9999938607215881
 0.9999706745147705
 0.999746561050415
 0.9998345971107483
 0.9999919533729553
 0.9999831318855286
 0.9999982714653015
 0.9993754625320435
I
----------
--------
 0.9999863505363464
 0.9994649291038513
 0.9998263120651245
 0.9991034269332886
 ||| 0.9999986290931702 |||
 0.9994115829467773
 0.9997011423110962
 0.9994049072265625
 0.9998673796653748
 0.9993433952331543
 0.9998159408569336
 (( 0.999918520450592 ))
 0.9999952912330627
 0.9988824129104614
 0.999319851398468
 0.9998739361763
 0.9999598264694214
 0.9995786547660828
 0.9998677968978882
 0.9996010661125183
 0.9993621110916138
 0.9999986290931702
 0.9996166229248047
 0.9994730949401855
I
----------
--------
 0.9999574422836304
 0.9994681477546692
 0.9998036623001099
 0.9988915920257568
 0.999952495098114
 0.9994181394577026
 0.9988507628440857
 0.9989929795265198
 0.9998475313186646
 0.9994545578956604
 0.9996650218963623
 0.9999028444290161
 (( 0.9993981719017029 ))
 0.9991962313652039
 0.999356210231781
 0.999805748462677
 0.9994895458221436
 0.9994133114814758
 0.9999731183052063
 0.9996141195297241
 0.9994232058525085
 ||| 0.9999966621398926 |||
 0.9994921088218689
 0.9981164932250977
I
----------
--------
 0.9996911287307739
 0.9999915361404419
 0.9995025396347046
 0.9999507665634155
 0.9998604655265808
 ||| 0.9999997019767761 |||
 0.9999192953109741
 0.999998927116394
 0.9998536705970764
 0.9999918937683105
 0.9999659061431885
 0.9997875690460205
 0.9995623826980591
 (( 0.9999837279319763 ))
 0.999992311000824
 0.9997438788414001
 0.9999945163726807
 0.9999599456787109
 0.9998472332954407
 0.9999746680259705
 0.9999992847442627
 0.9995825886726379
 0.9999806880950928
 0.9995846152305603
I
----------
--------
 0.9995577335357666
 0.9999911189079285
 0.999335765838623
 0.9999860525131226
 0.9995585680007935
 0.9999917149543762
 0.9999041557312012
 ||| 0.9999974370002747 |||
 0.9996237754821777
 0.9999840259552002
 0.9998173117637634
 0.9996742010116577
 0.9993800520896912
 0.9999874234199524
 (( 0.9999927878379822 ))
 0.9998050332069397
 0.99978107213974
 0.9999951720237732
 0.999752402305603
 0.9999303221702576
 0.9999147057533264
 0.9994291663169861
 0.999878466129303
 0.9997106194496155
I
----------
--------
 0.9999973773956299
 0.9998635053634644
 ||| 0.9999995827674866 |||
 0.9994983673095703
 0.9999426603317261
 0.9997537136077881
 0.9993433356285095
 0.9997179508209229
 0.9999623894691467
 0.9994630217552185
 0.9998225569725037
 0.9999979734420776
 0.9999692440032959
 0.9993306398391724
 0.9996581673622131
 (( 0.9999696612358093 ))
 0.9999586939811707
 0.9996903538703918
 0.9999985694885254
 0.9998261332511902
 0.999929666519165
 0.9999750256538391
 0.9999887943267822
 0.9987600445747375
I
----------
--------
 ||| 0.9999994039535522 |||
 0.9997177124023438
 0.9999525547027588
 0.999440610408783
 0.9999949932098389
 0.999678373336792
 0.9999313354492188
 0.9993956685066223
 0.9998053312301636
 0.998816967010498
 0.9999414682388306
 0.9999810457229614
 0.999692440032959
 0.999554455280304
 0.9995699524879456
 0.999464750289917
 (( 0.999998927116394 ))
 0.9997984170913696
 0.9999961256980896
 0.9997913241386414
 0.9997158050537109
 0.9999733567237854
 0.9997912645339966
 0.9986729025840759
I
----------
--------
 0.9999482035636902
 0.9997295141220093
 0.9999973773956299
 0.9997120499610901
 0.999991238117218
 0.9998974204063416
 0.9999821782112122
 0.99952632188797
 0.9999964237213135
 0.9998452663421631
 0.9999791979789734
 0.9999932646751404
 0.999891459941864
 0.999793529510498
 0.9999558925628662
 0.9999956488609314
 ||| 0.9999984502792358 |||
 (( 0.9999437928199768 ))
 0.9999216794967651
 0.9999745488166809
 0.9998992681503296
 0.9998509287834167
 0.9998812675476074
 0.9999043345451355
I
----------
--------
 ||| 0.9999971389770508 |||
 0.9993683695793152
 0.9999969005584717
 0.9992098212242126
 0.999986469745636
 0.9996415376663208
 0.9998947381973267
 0.9992952346801758
 0.9999676942825317
 0.9986828565597534
 0.9998278617858887
 0.9999781250953674
 0.9999946355819702
 0.9994674324989319
 0.999542236328125
 0.9999527931213379
 0.9999373555183411
 0.9995681047439575
 (( 0.999994695186615 ))
 0.9997907280921936
 0.999650239944458
 0.999944269657135
 0.9995810389518738
 0.9985317587852478
I
----------
--------
 0.9994646310806274
 0.9999716281890869
 0.9992227554321289
 ||| 0.9999991655349731 |||
 0.99946528673172
 0.9999579787254333
 0.9998552799224854
 0.999984860420227
 0.9991920590400696
 0.9999212026596069
 0.9998734593391418
 0.9991494417190552
 0.9999659657478333
 0.9997243285179138
 0.999898374080658
 0.9999932646751404
 0.9994762539863586
 0.9999948143959045
 0.9993836283683777
 (( 0.9999274015426636 ))
 0.9999798536300659
 0.9997656345367432
 0.9999713897705078
 0.9999534487724304
I
----------
--------
 0.9996933937072754
 0.9999579191207886
 0.9997763633728027
 0.9999496936798096
 0.9997201561927795
 0.9999985694885254
 0.9999263286590576
 0.9998608231544495
 0.9999843239784241
 0.9999938011169434
 0.9999152421951294
 0.9998434782028198
 0.9999828338623047
 0.999981164932251
 0.9999933242797852
 0.999756932258606
 0.9999160170555115
 0.9999688267707825
 0.9999203085899353
 0.9999747276306152
 (( 0.999945878982544 ))
 0.9996730089187622
 0.9999971389770508
 ||| 0.9999999403953552 |||
I
----------
--------
 0.9999927878379822
 0.9996659755706787
 ||| 0.9999998807907104 |||
 0.9991613030433655
 0.9998405575752258
 0.9995025396347046
 0.9997343420982361
 0.9992336630821228
 0.9996306300163269
 0.9995633363723755
 0.9998417496681213
 0.9999353885650635
 0.9994827508926392
 0.9994887709617615
 0.9993199110031128
 0.9991970658302307
 0.9995684623718262
 0.999522864818573
 0.9999988675117493
 0.9997051358222961
 0.9995073080062866
 (( 0.9999951720237732 ))
 0.9996542930603027
 0.9982677698135376
I
----------
--------
 0.9999370574951172
 0.9998975396156311
 0.9999067187309265
 0.9996371269226074
 0.9999911189079285
 0.9998228549957275
 0.9995020627975464
 0.9998189806938171
 0.9999114274978638
 0.9999035000801086
 0.9999949932098389
 ||| 0.9999961256980896 |||
 0.9998293519020081
 0.9987947940826416
 0.9999200701713562
 0.9996523261070251
 0.9998828768730164
 0.9997966289520264
 0.9999793767929077
 0.9996045827865601
 0.9999739527702332
 0.9999008178710938
 (( 0.9999725818634033 ))
 0.9989803433418274
I
----------
--------
 0.9984822869300842
 0.9995104670524597
 0.9983992576599121
 0.9998213648796082
 0.9986921548843384
 0.9995830655097961
 0.9993331432342529
 ||| 0.9998785853385925 |||
 0.9990564584732056
 0.9995400309562683
 0.9991405606269836
 0.9981870651245117
 0.9984046220779419
 0.9997273087501526
 0.9994925856590271
 0.9991043210029602
 0.9997284412384033
 0.9994303584098816
 0.9990801811218262
 0.9993727803230286
 0.999582827091217
 0.9987433552742004
 0.9996808767318726
 (( 0.9995692372322083 ))
I
----------
Epoch avg precision: [0.545138888888889, 0.545138888888889, 0.545138888888889, 0.545138888888889, 0.545138888888889]
Epoch avg recall: [1.0, 1.0, 1.0, 1.0, 1.0]
Epoch avg balanced accuracy: [0.5, 0.5, 0.5, 0.5, 0.5]
Epoch avg f-score: [0.6831069918556075, 0.6831069918556075, 0.6831069918556075, 0.6831069918556075, 0.6831069918556075]
Epoch one-shot accuracy: 0.08333333333333333



















Epoch [2/200]:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 17/19 [00:37<00:04,  2.18s/it, loss=0.25]
lr 0.0009801
Epoch 2

 Current loss 0.25222758949013196
Zero precision: [0, 0, 0, 0, 0]
Zero recall: [0, 0, 0, 0, 0]
Zero acc: [0, 0, 0, 0, 0]
--------
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 0.9999999403953552
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0000001192092896 |||
 (( 1.0000001192092896 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0000001192092896 |||
 1.0
 1.0000001192092896
 1.0000001192092896
 (( 1.0 ))
 1.0
 1.0000001192092896
 1.0000001192092896
 1.0
 1.0000001192092896
 1.0
 1.0000001192092896
 1.0
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
 1.0
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 0.9999999403953552
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0000001192092896 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
 1.0
 1.0
 1.0
 1.0000001192092896
 1.0000001192092896
C
----------
--------
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0000001192092896
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0000001192092896
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
 1.0
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0000001192092896
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
 1.0
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
 1.0000001192092896
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0000001192092896
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 1.0
 ||| 1.0000001192092896 |||
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0
 1.0000001192092896
 1.0
 1.0000001192092896
 1.0
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0
 (( 1.0000001192092896 ))
 1.0
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
I
----------
--------
 ||| 1.0000001192092896 |||
 1.0
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0
 1.0
 (( 1.0000001192092896 ))
 1.0000001192092896
 1.0
 1.0000001192092896
 1.0
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 ||| 1.0000001192092896 |||
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0000001192092896
I
----------
--------
 1.0
 ||| 1.0000001192092896 |||
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 1.0000001192092896
 (( 1.0000001192092896 ))
I
----------
Epoch avg precision: [0.564236111111111, 0.564236111111111, 0.564236111111111, 0.564236111111111, 0.564236111111111]
Epoch avg recall: [1.0, 1.0, 1.0, 1.0, 1.0]
Epoch avg balanced accuracy: [0.5, 0.5, 0.5, 0.5, 0.5]
Epoch avg f-score: [0.7195678271308523, 0.7195678271308523, 0.7195678271308523, 0.7195678271308523, 0.7195678271308523]
Epoch one-shot accuracy: 0.041666666666666664



















Epoch [3/200]:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎      | 18/19 [00:40<00:02,  2.21s/it, loss=0.25]
lr 0.000970299
Epoch 3

 Current loss 0.24143769349845198
Zero precision: [0, 0, 0, 0, 0]
Zero recall: [0, 0, 0, 0, 0]
Zero acc: [0, 0, 0, 0, 0]
--------
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
C
----------
--------
 ||| 1.0000001192092896 |||
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0000001192092896 |||
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0000001192092896
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
I
----------
--------
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 ||| 1.0000001192092896 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
I
----------
Epoch avg precision: [0.4461805555555556, 0.4461805555555556, 0.4461805555555556, 0.4461805555555556, 0.4461805555555556]
Epoch avg recall: [1.0, 1.0, 1.0, 1.0, 1.0]
Epoch avg balanced accuracy: [0.5, 0.5, 0.5, 0.5, 0.5]
Epoch avg f-score: [0.611839722969291, 0.611839722969291, 0.611839722969291, 0.611839722969291, 0.611839722969291]
Epoch one-shot accuracy: 0.041666666666666664


















Epoch [4/200]:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋             | 17/19 [00:36<00:04,  2.15s/it, loss=0.273]
lr 0.0009605960099999999
Epoch 4

 Current loss 0.24192143962848298
Zero precision: [0, 0, 0, 0, 0]
Zero recall: [0, 0, 0, 0, 0]
Zero acc: [0, 0, 0, 0, 0]
--------
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
C
----------
--------
 ||| 1.0 |||
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
I
----------
Epoch avg precision: [0.4427083333333333, 0.4427083333333333, 0.4427083333333333, 0.4427083333333333, 0.4427083333333333]
Epoch avg recall: [1.0, 1.0, 1.0, 1.0, 1.0]
Epoch avg balanced accuracy: [0.5, 0.5, 0.5, 0.5, 0.5]
Epoch avg f-score: [0.6125317311256256, 0.6125317311256256, 0.6125317311256256, 0.6125317311256256, 0.6125317311256256]
Epoch one-shot accuracy: 0.041666666666666664


















Epoch [5/200]:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋             | 17/19 [00:36<00:04,  2.16s/it, loss=0.305]
lr 0.0009509900498999999
Epoch 5

 Current loss 0.2501451238390093
Zero precision: [0, 0, 0, 0, 0]
Zero recall: [0, 0, 0, 0, 0]
Zero acc: [0, 0, 0, 0, 0]
--------
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
C
----------
--------
 ||| 1.0 |||
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
 1.0
I
----------
--------
 ||| 1.0 |||
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 1.0
 (( 1.0 ))
I
----------
Epoch avg precision: [0.611111111111111, 0.611111111111111, 0.611111111111111, 0.611111111111111, 0.611111111111111]
Epoch avg recall: [1.0, 1.0, 1.0, 1.0, 1.0]
Epoch avg balanced accuracy: [0.5, 0.5, 0.5, 0.5, 0.5]
Epoch avg f-score: [0.7571701957136131, 0.7571701957136131, 0.7571701957136131, 0.7571701957136131, 0.7571701957136131]
Epoch one-shot accuracy: 0.041666666666666664







Traceback (most recent call last):
  File "/home/adriansegura/Desktop/RUG/CattleNet/convnext/setup.py", line 264, in <module>
    model,res_balanced_acc,res_f_score = train(d_loader=data_loader,dataset_validation=dataset_validation)
  File "/home/adriansegura/Desktop/RUG/CattleNet/convnext/setup.py", line 122, in train
    out1,out2 = model(imgs1,imgs2)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/adriansegura/Desktop/RUG/CattleNet/convnext/cattleNetTest.py", line 50, in forward
    out1 = self.forward_once(input1)
  File "/home/adriansegura/Desktop/RUG/CattleNet/convnext/cattleNetTest.py", line 44, in forward_once
    x = self.convnext_tiny(input)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torchvision/models/convnext.py", line 186, in forward
    return self._forward_impl(x)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torchvision/models/convnext.py", line 180, in _forward_impl
    x = self.features(x)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torchvision/models/convnext.py", line 73, in forward
    result = self.layer_scale * self.block(input)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 447, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/adriansegura/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 443, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
KeyboardInterrupt